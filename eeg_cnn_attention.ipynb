{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWL9ddXYknNj"
      },
      "outputs": [],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJQF9MY6kxEU"
      },
      "outputs": [],
      "source": [
        "!pip install pymatreader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4WeWoCCkzyu"
      },
      "outputs": [],
      "source": [
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mne\n",
        "import os\n",
        "from scipy.signal import resample\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhFx437Ak9NJ"
      },
      "outputs": [],
      "source": [
        "# Load the participant metadata file\n",
        "metadata_path = \"/content/participants.tsv\"\n",
        "metadata = pd.read_csv(metadata_path, sep=\"\\t\") \n",
        "\n",
        "print(metadata.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms9wN1ahk_Qn"
      },
      "outputs": [],
      "source": [
        "group_mapping = {'A': 0, 'F': 1, 'C': 2}\n",
        "metadata['Group'] = metadata['Group'].map(group_mapping)\n",
        "\n",
        "# Check distribution of each Group \n",
        "group_counts = metadata['Group'].value_counts()\n",
        "print(f\"Group distribution:\\n{group_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npxhALb0lHpw"
      },
      "outputs": [],
      "source": [
        "gender_mapping = {'M': 0, 'F': 1}\n",
        "metadata['Gender'] = metadata['Gender'].map(gender_mapping)\n",
        "\n",
        "# Check distribution of Gender\n",
        "group_counts = metadata['Gender'].value_counts()\n",
        "print(f\"Gender distribution:\\n{group_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQRy7XR_lS8U"
      },
      "outputs": [],
      "source": [
        "# Function to downsample EEG data\n",
        "def downsample_data(data, original_sfreq, target_sfreq):\n",
        "    num_samples = int(data.shape[1] * target_sfreq / original_sfreq)\n",
        "    downsampled_data = resample(data, num_samples, axis=1)\n",
        "    return downsampled_data\n",
        "\n",
        "# Function to perform epoching with 50% overlap and 4s windows\n",
        "def create_epochs(data, sfreq, epoch_duration=4, overlap=0.5):\n",
        "    samples_per_epoch = int(epoch_duration * sfreq)\n",
        "    step_samples = int(samples_per_epoch * (1 - overlap))  # 50% overlap\n",
        "\n",
        "    num_epochs = max(0, (data.shape[1] - samples_per_epoch) // step_samples + 1)\n",
        "    epochs = []\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        start = i * step_samples\n",
        "        end = start + samples_per_epoch\n",
        "        epochs.append(data[:, start:end])\n",
        "\n",
        "    return np.array(epochs)\n",
        "\n",
        "# Function to load EEG data, downsample, epoch, and extract metadata\n",
        "def load_and_preprocess_eeg_with_metadata(eeg_folder, metadata_df, target_sfreq=128, epoch_duration=4, overlap=0.5):\n",
        "    eeg_files = [f for f in os.listdir(eeg_folder) if f.endswith('.set')]\n",
        "    all_epochs = []\n",
        "    all_groups = []\n",
        "    all_ages = []\n",
        "    all_mmse = []\n",
        "    all_genders = []\n",
        "    all_patient_ids = []\n",
        "\n",
        "    for eeg_file in eeg_files:\n",
        "        file_path = os.path.join(eeg_folder, eeg_file)\n",
        "\n",
        "        # Load the EEG data\n",
        "        eeg_data = mne.io.read_raw_eeglab(file_path, preload=True)\n",
        "        original_sfreq = eeg_data.info['sfreq']\n",
        "        data, _ = eeg_data.get_data(return_times=True)  # Data shape: (n_channels, n_timepoints)\n",
        "\n",
        "        # Downsample the data\n",
        "        downsampled_data = downsample_data(data, original_sfreq, target_sfreq)\n",
        "\n",
        "        # Epoch the downsampled data\n",
        "        epochs = create_epochs(downsampled_data, target_sfreq, epoch_duration, overlap)\n",
        "\n",
        "        # Extract participant ID (assuming filename format is 'sub-XX_something.set')\n",
        "        participant_id = eeg_file.split('_')[0]\n",
        "\n",
        "        # Retrieve group, age, and MMSE from metadata\n",
        "        participant_info = metadata_df[metadata_df['participant_id'] == participant_id]\n",
        "\n",
        "        if not participant_info.empty:\n",
        "            group = participant_info.iloc[0]['Group']\n",
        "            age = participant_info.iloc[0]['Age']\n",
        "            mmse = participant_info.iloc[0]['MMSE']\n",
        "            gender = participant_info.iloc[0]['Gender']\n",
        "        else:\n",
        "            group, age, mmse, gender = 'Unknown', np.nan, np.nan, np.nan  # Assign NaN for missing values\n",
        "\n",
        "        # Append values for each epoch\n",
        "        if epochs.shape[0] > 0:\n",
        "            all_epochs.append(epochs)\n",
        "            all_groups.extend([group] * epochs.shape[0])\n",
        "            all_ages.extend([age] * epochs.shape[0])\n",
        "            all_mmse.extend([mmse] * epochs.shape[0])\n",
        "            all_genders.extend([gender] * epochs.shape[0])\n",
        "            all_patient_ids.extend([participant_id] * epochs.shape[0])\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    all_epochs = np.vstack(all_epochs) if all_epochs else np.array([])\n",
        "    all_groups = np.array(all_groups)\n",
        "    all_ages = np.array(all_ages, dtype=float)\n",
        "    all_mmse = np.array(all_mmse, dtype=float)\n",
        "    all_genders = np.array(all_genders, dtype=int)\n",
        "    all_patient_ids = np.array(all_patient_ids)\n",
        "\n",
        "    return all_epochs, all_groups, all_ages, all_mmse, all_genders, all_patient_ids\n",
        "\n",
        "# Load the EEG data\n",
        "eeg_folder = '/content/derivatives'  # Path to EEG files\n",
        "target_sfreq = 128\n",
        "epochs, groups, ages, mmse_scores, genders, patient_ids = load_and_preprocess_eeg_with_metadata(eeg_folder, metadata, target_sfreq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yE_k4IRlZbE"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Normalize EEG data\n",
        "scaler = StandardScaler()\n",
        "epochs_flattened = epochs.reshape(epochs.shape[0], -1)  # Flatten the epochs for scaling\n",
        "epochs_normalized = scaler.fit_transform(epochs_flattened)  # Normalize\n",
        "epochs_normalized = epochs_normalized.reshape(epochs.shape[0], 19, 512)  # Reshape back to (epochs, channels, timepoints)\n",
        "\n",
        "# Normalize the additional features (age, MMSE)\n",
        "age_scaler = StandardScaler()\n",
        "ages_normalized = age_scaler.fit_transform(ages.reshape(-1, 1))\n",
        "\n",
        "mmse_scaler = StandardScaler()\n",
        "mmse_normalized = mmse_scaler.fit_transform(mmse_scores.reshape(-1, 1))\n",
        "\n",
        "# Gender doesn't require scaling, so we use it as it is\n",
        "genders_tensor = torch.tensor(genders, dtype=torch.long)\n",
        "\n",
        "# Preparing the additional features (age, MMSE, gender) for training and testing\n",
        "ages_tensor = torch.tensor(ages_normalized, dtype=torch.float).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "mmse_tensor = torch.tensor(mmse_normalized, dtype=torch.float).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "genders_tensor = torch.tensor(genders, dtype=torch.long).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "print(ages_tensor.shape)\n",
        "print(mmse_tensor.shape)\n",
        "print(genders_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3-ueuzClleB"
      },
      "outputs": [],
      "source": [
        "# CNN Architecture (with Attention Mechanism)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Block 1 (Conv Layer)\n",
        "        self.block1_conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.block1_conv2 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Attention Layer (for EEG data)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=64, num_heads=4, dropout=0.1)\n",
        "\n",
        "        # Block 2 (Conv Layer)\n",
        "        self.block2_conv1 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.block2_conv2 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 + 3, 512)  # 128 from Conv output + 3 metadata features\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x, age, mmse, gender):\n",
        "        # Block 1 (Convolutional Layers)\n",
        "        x = F.relu(self.block1_conv1(x))\n",
        "        x = F.relu(self.block1_conv2(x))\n",
        "\n",
        "        # Attention mechanism\n",
        "        x = x.permute(2, 0, 1)  # (batch, channels, seq_len) â†’ (seq_len, batch, channels)\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        x = attn_output.permute(1, 2, 0)  # back to (batch, channels, seq_len)\n",
        "\n",
        "        # Block 2 (Convolutional Layers)\n",
        "        x = F.relu(self.block2_conv1(x))\n",
        "        x = F.relu(self.block2_conv2(x))\n",
        "\n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x).squeeze(-1)  # Shape: (batch_size, 128)\n",
        "\n",
        "        # Flatten metadata\n",
        "        age = age.view(age.size(0), -1)\n",
        "        mmse = mmse.view(mmse.size(0), -1)\n",
        "        gender = gender.view(gender.size(0), -1)\n",
        "\n",
        "        # Concatenate all features\n",
        "        combined_features = torch.cat([x, age, mmse, gender], dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(combined_features))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Data Augmentation (add noise to input data)\n",
        "def augment_data(x, noise_factor=0.1):\n",
        "    # Adding random noise to the input\n",
        "    noise = torch.randn_like(x) * noise_factor\n",
        "    return x + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MooaPGhWl6vc"
      },
      "outputs": [],
      "source": [
        "# Perform Leave-One-Patient-Out Cross-Validation\n",
        "patients = np.unique(patient_ids)\n",
        "accuracies = []\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for patient in patients:\n",
        "    # Split the data into training and testing based on patient ID\n",
        "    train_mask = patient_ids != patient\n",
        "    test_mask = patient_ids == patient\n",
        "\n",
        "    # Prepare training and testing data\n",
        "    X_train, y_train = epochs_normalized[train_mask], groups[train_mask]\n",
        "    X_test, y_test = epochs_normalized[test_mask], groups[test_mask]\n",
        "\n",
        "    # Prepare the additional features (age, MMSE, gender) for training and testing\n",
        "    age_train, mmse_train, gender_train = ages_tensor[train_mask], mmse_tensor[train_mask], genders_tensor[train_mask]\n",
        "    age_test, mmse_test, gender_test = ages_tensor[test_mask], mmse_tensor[test_mask], genders_tensor[test_mask]\n",
        "\n",
        "    # Convert to tensors and move to the device\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "    age_train_tensor = age_train.to(device)\n",
        "    mmse_train_tensor = mmse_train.to(device)\n",
        "    gender_train_tensor = gender_train.to(device)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "    age_test_tensor = age_test.to(device)\n",
        "    mmse_test_tensor = mmse_test.to(device)\n",
        "    gender_test_tensor = gender_test.to(device)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor, age_train_tensor, mmse_train_tensor, gender_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor, age_test_tensor, mmse_test_tensor, gender_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = CNN(input_channels=19, num_classes=3).to(device)\n",
        "\n",
        "    # Define loss function and optimizer with weight decay\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 regularization\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate scheduler\n",
        "\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for epoch in range(5):  # Increase this number to 10-50 for better training\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels, age, mmse, gender in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            age, mmse, gender = age.to(device), mmse.to(device), gender.to(device)\n",
        "\n",
        "            # Apply augmentation here\n",
        "            inputs = augment_data(inputs)\n",
        "            # print(inputs.shape)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, age, mmse, gender)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()  # Step the learning rate scheduler\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, age, mmse, gender in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            age, mmse, gender = age.to(device), mmse.to(device), gender.to(device)\n",
        "            outputs = model(inputs, age, mmse, gender)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Collect predictions and labels for later metric calculations\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Patient {patient}: Test Accuracy: {accuracy}%\")\n",
        "\n",
        "# Calculate metrics after loop\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "# Print overall performance metrics\n",
        "print(f\"Average Accuracy: {np.mean(accuracies)}%\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx8-EJ4Slq4C"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "group_accuracies = defaultdict(list)\n",
        "\n",
        "for i, patient in enumerate(patients):\n",
        "    patient_group = groups[patient_ids == patient][0]  # All epochs for a patient have the same group\n",
        "    group_accuracies[patient_group].append(accuracies[i])\n",
        "\n",
        "# Print mean accuracy per group\n",
        "for group, accs in group_accuracies.items():\n",
        "    print(f\"Group {group} Mean Accuracy: {np.mean(accs):.2f}% (n={len(accs)})\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
