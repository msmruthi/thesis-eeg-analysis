{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef850b_BbsO_"
      },
      "outputs": [],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00FgCZMsbxLe"
      },
      "outputs": [],
      "source": [
        "!pip install pymatreader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8kxNmV9b0G0"
      },
      "outputs": [],
      "source": [
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mne\n",
        "import pandas as pd\n",
        "import os\n",
        "from scipy.signal import resample\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL0AvEBsb0eR"
      },
      "outputs": [],
      "source": [
        "# Load the participant metadata file\n",
        "metadata_path = \"/content/participants.tsv\"\n",
        "metadata = pd.read_csv(metadata_path, sep=\"\\t\")\n",
        "\n",
        "print(metadata.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6q3kkn6b5mI"
      },
      "outputs": [],
      "source": [
        "group_mapping = {'A': 0, 'F': 1, 'C': 2}\n",
        "metadata['Group'] = metadata['Group'].map(group_mapping)\n",
        "\n",
        "# Check distribution of each Group \n",
        "group_counts = metadata['Group'].value_counts()\n",
        "print(f\"Group distribution:\\n{group_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-ZeH_2Tb-mu"
      },
      "outputs": [],
      "source": [
        "gender_mapping = {'M': 0, 'F': 1}\n",
        "metadata['Gender'] = metadata['Gender'].map(gender_mapping)\n",
        "\n",
        "# Check distribution of Gender\n",
        "group_counts = metadata['Gender'].value_counts()\n",
        "print(f\"Gender distribution:\\n{group_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g63KLSOxcKRz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to downsample EEG data\n",
        "def downsample_data(data, original_sfreq, target_sfreq):\n",
        "    num_samples = int(data.shape[1] * target_sfreq / original_sfreq)\n",
        "    downsampled_data = resample(data, num_samples, axis=1)\n",
        "    return downsampled_data\n",
        "\n",
        "# Function to perform epoching with 50% overlap and 4s windows\n",
        "def create_epochs(data, sfreq, epoch_duration=4, overlap=0.5):\n",
        "    samples_per_epoch = int(epoch_duration * sfreq)\n",
        "    step_samples = int(samples_per_epoch * (1 - overlap))  # 50% overlap\n",
        "\n",
        "    num_epochs = max(0, (data.shape[1] - samples_per_epoch) // step_samples + 1)\n",
        "    epochs = []\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        start = i * step_samples\n",
        "        end = start + samples_per_epoch\n",
        "        epochs.append(data[:, start:end])\n",
        "\n",
        "    return np.array(epochs)\n",
        "\n",
        "# Function to load EEG data, downsample, epoch, and extract metadata\n",
        "def load_and_preprocess_eeg_with_metadata(eeg_folder, metadata_df, target_sfreq=128, epoch_duration=4, overlap=0.5):\n",
        "    eeg_files = [f for f in os.listdir(eeg_folder) if f.endswith('.set')]\n",
        "    all_epochs = []\n",
        "    all_groups = []\n",
        "    all_ages = []\n",
        "    all_mmse = []\n",
        "    all_genders = []\n",
        "    all_patient_ids = []\n",
        "\n",
        "    for eeg_file in eeg_files:\n",
        "        file_path = os.path.join(eeg_folder, eeg_file)\n",
        "\n",
        "        # Load the EEG data\n",
        "        eeg_data = mne.io.read_raw_eeglab(file_path, preload=True)\n",
        "        original_sfreq = eeg_data.info['sfreq']\n",
        "        data, _ = eeg_data.get_data(return_times=True)  # Data shape: (n_channels, n_timepoints)\n",
        "\n",
        "        # Downsample the data\n",
        "        downsampled_data = downsample_data(data, original_sfreq, target_sfreq)\n",
        "\n",
        "        # Epoch the downsampled data\n",
        "        epochs = create_epochs(downsampled_data, target_sfreq, epoch_duration, overlap)\n",
        "\n",
        "        # Extract participant ID (filename format is 'sub-XXX_task-eyesclosed_eeg.set')\n",
        "        participant_id = eeg_file.split('_')[0]\n",
        "\n",
        "        # Retrieve group, age, and MMSE from metadata\n",
        "        participant_info = metadata_df[metadata_df['participant_id'] == participant_id]\n",
        "\n",
        "        if not participant_info.empty:\n",
        "            group = participant_info.iloc[0]['Group']\n",
        "            age = participant_info.iloc[0]['Age']\n",
        "            mmse = participant_info.iloc[0]['MMSE']\n",
        "            gender = participant_info.iloc[0]['Gender']\n",
        "        else:\n",
        "            group, age, mmse, gender = 'Unknown', np.nan, np.nan, np.nan  # Assign NaN for missing values\n",
        "\n",
        "        # Append values for each epoch\n",
        "        if epochs.shape[0] > 0:\n",
        "            all_epochs.append(epochs)\n",
        "            all_groups.extend([group] * epochs.shape[0])\n",
        "            all_ages.extend([age] * epochs.shape[0])\n",
        "            all_mmse.extend([mmse] * epochs.shape[0])\n",
        "            all_genders.extend([gender] * epochs.shape[0])\n",
        "            all_patient_ids.extend([participant_id] * epochs.shape[0])\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    all_epochs = np.vstack(all_epochs) if all_epochs else np.array([])\n",
        "    all_groups = np.array(all_groups)\n",
        "    all_ages = np.array(all_ages, dtype=float)\n",
        "    all_mmse = np.array(all_mmse, dtype=float)\n",
        "    all_genders = np.array(all_genders, dtype=int)\n",
        "    all_patient_ids = np.array(all_patient_ids)\n",
        "\n",
        "    return all_epochs, all_groups, all_ages, all_mmse, all_genders, all_patient_ids\n",
        "\n",
        "# Load the EEG data\n",
        "eeg_folder = '/content/derivatives'  # Path to EEG files\n",
        "target_sfreq = 128\n",
        "epochs, groups, ages, mmse_scores, genders, patient_ids = load_and_preprocess_eeg_with_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0feQN4gzcax8"
      },
      "outputs": [],
      "source": [
        "# Assuming your EEG, age, MMSE, and gender data are already preprocessed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Normalize EEG data\n",
        "scaler = StandardScaler()\n",
        "epochs_flattened = epochs.reshape(epochs.shape[0], -1)  # Flatten the epochs for scaling\n",
        "epochs_normalized = scaler.fit_transform(epochs_flattened)  # Normalize\n",
        "epochs_normalized = epochs_normalized.reshape(epochs.shape[0], 19, 512)  # Reshape back to (epochs, channels, timepoints)\n",
        "\n",
        "# Normalize the additional features (age, MMSE)\n",
        "age_scaler = StandardScaler()\n",
        "ages_normalized = age_scaler.fit_transform(ages.reshape(-1, 1))\n",
        "\n",
        "mmse_scaler = StandardScaler()\n",
        "mmse_normalized = mmse_scaler.fit_transform(mmse_scores.reshape(-1, 1))\n",
        "\n",
        "# Gender doesn't require scaling, so we just use it as-is\n",
        "genders_tensor = torch.tensor(genders, dtype=torch.long)\n",
        "\n",
        "# Prepare the additional features (age, MMSE, gender) for training and testing\n",
        "ages_tensor = torch.tensor(ages_normalized, dtype=torch.float).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "mmse_tensor = torch.tensor(mmse_normalized, dtype=torch.float).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "genders_tensor = torch.tensor(genders, dtype=torch.long).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "\n",
        "\n",
        "# ResNet Model with Dropout, Weight Decay, and Data Augmentation\n",
        "class ResNet1D(nn.Module):\n",
        "\n",
        "    # Initialize model with given input channels and number of output classes\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(ResNet1D, self).__init__()\n",
        "\n",
        "        # Block 1 - 2 convolutions and a residual connection, followed by dropout for regularization\n",
        "        self.block1_conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.block1_conv2 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.block1_residual = nn.Conv1d(input_channels, 64, kernel_size=1, stride=1, padding=0)\n",
        "        self.dropout1 = nn.Dropout(0.5)  # 50% dropout in Block 1\n",
        "\n",
        "        # Same for Block 2 and 3, but with increasing channels: 64→128→256\n",
        "        # Block 2\n",
        "        self.block2_conv1 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.block2_conv2 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.block2_residual = nn.Conv1d(64, 128, kernel_size=1, stride=1, padding=0)\n",
        "        self.dropout2 = nn.Dropout(0.5)  # 50% dropout in Block 2\n",
        "\n",
        "        # Block 3\n",
        "        self.block3_conv1 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.block3_conv2 = nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.block3_residual = nn.Conv1d(128, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.dropout3 = nn.Dropout(0.5)  # 50% dropout in Block 3\n",
        "\n",
        "        # Global Average Pooling\n",
        "        # Reduce the time dimension to one by averaging\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # First FC layer combines CNN output with 3 metadata features\n",
        "        self.fc1 = nn.Linear(256 + 3, 512)  # 256 from ResNet output + 3 features (age, MMSE, gender)\n",
        "        self.fc2 = nn.Linear(512, num_classes)  # Final output layer\n",
        "\n",
        "    # Accept EEG input and metadata\n",
        "    def forward(self, x, age, mmse, gender):\n",
        "        # Block 1\n",
        "        residual = x\n",
        "        x = self.block1_conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.block1_conv2(x)\n",
        "        x = F.relu(x)\n",
        "        residual = self.block1_residual(residual)\n",
        "        x = x + residual  # Residual connection\n",
        "        x = self.dropout1(x)  # Dropout after Block 1\n",
        "\n",
        "        # Block 2\n",
        "        residual = x\n",
        "        x = self.block2_conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.block2_conv2(x)\n",
        "        x = F.relu(x)\n",
        "        residual = self.block2_residual(residual)\n",
        "        x = x + residual  # Residual connection\n",
        "        x = self.dropout2(x)  # Dropout after Block 2\n",
        "\n",
        "        # Block 3\n",
        "        residual = x\n",
        "        x = self.block3_conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.block3_conv2(x)\n",
        "        x = F.relu(x)\n",
        "        residual = self.block3_residual(residual)\n",
        "        x = x + residual  # Residual connection\n",
        "        x = self.dropout3(x)  # Dropout after Block 3\n",
        "\n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x).squeeze(-1)  # Shape becomes [batch_size, channels]\n",
        "\n",
        "        # Concatenate additional features (age, MMSE, gender)\n",
        "        age_flattened = age.view(age.size(0), -1)\n",
        "        mmse_flattened = mmse.view(mmse.size(0), -1)\n",
        "        gender_flattened = gender.view(gender.size(0), -1)\n",
        "        combined_features = torch.cat([x, age_flattened, mmse_flattened, gender_flattened], dim=1)\n",
        "\n",
        "        # Fully connected layers after combining features\n",
        "        x = self.fc1(combined_features)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Data Augmentation (add Gaussian noise to EEG inputs to improve generalization)\n",
        "def augment_data(x, noise_factor=0.1):\n",
        "    # Adding random noise to the input\n",
        "    noise = torch.randn_like(x) * noise_factor\n",
        "    return x + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Silu14yhcvz1"
      },
      "outputs": [],
      "source": [
        "# Perform Leave-One-Patient-Out Cross-Validation\n",
        "patients = np.unique(patient_ids)\n",
        "accuracies = []\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for patient in patients:\n",
        "    # Split the data into training and testing based on patient ID\n",
        "    train_mask = patient_ids != patient\n",
        "    test_mask = patient_ids == patient\n",
        "\n",
        "    # Prepare training and testing data\n",
        "    X_train, y_train = epochs_normalized[train_mask], groups[train_mask]\n",
        "    X_test, y_test = epochs_normalized[test_mask], groups[test_mask]\n",
        "\n",
        "    # Prepare the additional features (age, MMSE, gender) for training and testing\n",
        "    age_train, mmse_train, gender_train = ages_tensor[train_mask], mmse_tensor[train_mask], genders_tensor[train_mask]\n",
        "    age_test, mmse_test, gender_test = ages_tensor[test_mask], mmse_tensor[test_mask], genders_tensor[test_mask]\n",
        "\n",
        "    # Convert to tensors and move to the device\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "    age_train_tensor = age_train.to(device)\n",
        "    mmse_train_tensor = mmse_train.to(device)\n",
        "    gender_train_tensor = gender_train.to(device)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "    age_test_tensor = age_test.to(device)\n",
        "    mmse_test_tensor = mmse_test.to(device)\n",
        "    gender_test_tensor = gender_test.to(device)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor, age_train_tensor, mmse_train_tensor, gender_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor, age_test_tensor, mmse_test_tensor, gender_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = ResNet1D(input_channels=19, num_classes=3).to(device)\n",
        "\n",
        "    # Define loss function and optimizer with weight decay\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 regularization\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate scheduler\n",
        "\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for epoch in range(5):  # Increase this number to 10-50 for better training\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels, age, mmse, gender in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            age, mmse, gender = age.to(device), mmse.to(device), gender.to(device)\n",
        "\n",
        "            # Apply augmentation here\n",
        "            inputs = augment_data(inputs)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, age, mmse, gender)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()  # Step the learning rate scheduler\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, age, mmse, gender in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            age, mmse, gender = age.to(device), mmse.to(device), gender.to(device)\n",
        "            outputs = model(inputs, age, mmse, gender)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Collect predictions and labels for performance metric calculations\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Patient {patient}: Test Accuracy: {accuracy}%\")\n",
        "\n",
        "# Calculate metrics after loop\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "# Print overall performance metrics\n",
        "print(f\"Average Accuracy: {np.mean(accuracies)}%\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
