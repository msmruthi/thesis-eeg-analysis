{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWL9ddXYknNj"
      },
      "outputs": [],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymatreader"
      ],
      "metadata": {
        "id": "kJQF9MY6kxEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "metadata": {
        "id": "A4WeWoCCkzyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the participant metadata file\n",
        "metadata_path = \"/content/participants.tsv\"\n",
        "metadata = pd.read_csv(metadata_path, sep=\"\\t\")  # Assuming it's tab-separated\n",
        "\n",
        "# Display the first few rows\n",
        "print(metadata.head())"
      ],
      "metadata": {
        "id": "jhFx437Ak9NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_mapping = {'A': 0, 'F': 1, 'C': 2}\n",
        "metadata['Group'] = metadata['Group'].map(group_mapping)\n",
        "\n",
        "# Check the distribution of the 'Group' column in your metadata DataFrame\n",
        "group_counts = metadata['Group'].value_counts()\n",
        "print(f\"Group distribution:\\n{group_counts}\")"
      ],
      "metadata": {
        "id": "ms9wN1ahk_Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_mapping = {'M': 0, 'F': 1}\n",
        "metadata['Gender'] = metadata['Gender'].map(gender_mapping)\n",
        "\n",
        "# Check the distribution of the 'Gender' column in your metadata DataFrame\n",
        "group_counts = metadata['Gender'].value_counts()\n",
        "print(f\"Gender distribution:\\n{group_counts}\")"
      ],
      "metadata": {
        "id": "npxhALb0lHpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import mne\n",
        "import pandas as pd\n",
        "import os\n",
        "from scipy.signal import resample\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "\n",
        "# Function to downsample EEG data\n",
        "def downsample_data(data, original_sfreq, target_sfreq):\n",
        "    num_samples = int(data.shape[1] * target_sfreq / original_sfreq)\n",
        "    downsampled_data = resample(data, num_samples, axis=1)\n",
        "    return downsampled_data\n",
        "\n",
        "# Function to perform epoching with 50% overlap and 4s windows\n",
        "def create_epochs(data, sfreq, epoch_duration=4, overlap=0.5):\n",
        "    samples_per_epoch = int(epoch_duration * sfreq)\n",
        "    step_samples = int(samples_per_epoch * (1 - overlap))  # 50% overlap\n",
        "\n",
        "    num_epochs = max(0, (data.shape[1] - samples_per_epoch) // step_samples + 1)\n",
        "    epochs = []\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        start = i * step_samples\n",
        "        end = start + samples_per_epoch\n",
        "        epochs.append(data[:, start:end])\n",
        "\n",
        "    return np.array(epochs)\n",
        "\n",
        "# Function to load EEG data, downsample, epoch, and extract metadata\n",
        "def load_and_preprocess_eeg_with_metadata(eeg_folder, metadata_df, target_sfreq=128, epoch_duration=4, overlap=0.5):\n",
        "    eeg_files = [f for f in os.listdir(eeg_folder) if f.endswith('.set')]\n",
        "    all_epochs = []\n",
        "    all_groups = []\n",
        "    all_ages = []\n",
        "    all_mmse = []\n",
        "    all_genders = []\n",
        "    all_patient_ids = []\n",
        "\n",
        "    for eeg_file in eeg_files:\n",
        "        file_path = os.path.join(eeg_folder, eeg_file)\n",
        "\n",
        "        # Load the EEG data\n",
        "        eeg_data = mne.io.read_raw_eeglab(file_path, preload=True)\n",
        "        original_sfreq = eeg_data.info['sfreq']\n",
        "        data, _ = eeg_data.get_data(return_times=True)  # Data shape: (n_channels, n_timepoints)\n",
        "\n",
        "        # Downsample the data\n",
        "        downsampled_data = downsample_data(data, original_sfreq, target_sfreq)\n",
        "\n",
        "        # Epoch the downsampled data\n",
        "        epochs = create_epochs(downsampled_data, target_sfreq, epoch_duration, overlap)\n",
        "\n",
        "        # Extract participant ID (assuming filename format is 'sub-XX_something.set')\n",
        "        participant_id = eeg_file.split('_')[0]\n",
        "\n",
        "        # Retrieve group, age, and MMSE from metadata\n",
        "        participant_info = metadata_df[metadata_df['participant_id'] == participant_id]\n",
        "\n",
        "        if not participant_info.empty:\n",
        "            group = participant_info.iloc[0]['Group']\n",
        "            age = participant_info.iloc[0]['Age']\n",
        "            mmse = participant_info.iloc[0]['MMSE']\n",
        "            gender = participant_info.iloc[0]['Gender']\n",
        "        else:\n",
        "            group, age, mmse, gender = 'Unknown', np.nan, np.nan, np.nan  # Assign NaN for missing values\n",
        "\n",
        "        # Append values for each epoch\n",
        "        if epochs.shape[0] > 0:\n",
        "            all_epochs.append(epochs)\n",
        "            all_groups.extend([group] * epochs.shape[0])\n",
        "            all_ages.extend([age] * epochs.shape[0])\n",
        "            all_mmse.extend([mmse] * epochs.shape[0])\n",
        "            all_genders.extend([gender] * epochs.shape[0])\n",
        "            all_patient_ids.extend([participant_id] * epochs.shape[0])\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    all_epochs = np.vstack(all_epochs) if all_epochs else np.array([])\n",
        "    all_groups = np.array(all_groups)\n",
        "    all_ages = np.array(all_ages, dtype=float)\n",
        "    all_mmse = np.array(all_mmse, dtype=float)\n",
        "    all_genders = np.array(all_genders, dtype=int)\n",
        "    all_patient_ids = np.array(all_patient_ids)\n",
        "\n",
        "    return all_epochs, all_groups, all_ages, all_mmse, all_genders, all_patient_ids\n",
        "\n",
        "# Load the EEG data\n",
        "eeg_folder = '/content/derivatives'  # Path to EEG files\n",
        "target_sfreq = 128\n",
        "epochs, groups, ages, mmse_scores, genders, patient_ids = load_and_preprocess_eeg_with_metadata(eeg_folder, metadata, target_sfreq)\n"
      ],
      "metadata": {
        "id": "LQRy7XR_lS8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming your EEG, age, MMSE, and gender data are already preprocessed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Normalize EEG data\n",
        "scaler = StandardScaler()\n",
        "epochs_flattened = epochs.reshape(epochs.shape[0], -1)  # Flatten the epochs for scaling\n",
        "epochs_normalized = scaler.fit_transform(epochs_flattened)  # Normalize\n",
        "epochs_normalized = epochs_normalized.reshape(epochs.shape[0], 19, 512)  # Reshape back to (epochs, channels, timepoints)\n",
        "\n",
        "# Normalize the additional features (age, MMSE)\n",
        "age_scaler = StandardScaler()\n",
        "ages_normalized = age_scaler.fit_transform(ages.reshape(-1, 1))\n",
        "\n",
        "mmse_scaler = StandardScaler()\n",
        "mmse_normalized = mmse_scaler.fit_transform(mmse_scores.reshape(-1, 1))\n",
        "\n",
        "# Gender doesn't require scaling, so we just use it as-is\n",
        "genders_tensor = torch.tensor(genders, dtype=torch.long)\n",
        "\n",
        "# Prepare the additional features (age, MMSE, gender) for training and testing\n",
        "ages_tensor = torch.tensor(ages_normalized, dtype=torch.float).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "mmse_tensor = torch.tensor(mmse_normalized, dtype=torch.float).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "genders_tensor = torch.tensor(genders, dtype=torch.long).unsqueeze(1)  # Shape becomes [34788, 1]\n",
        "print(ages_tensor.shape)\n",
        "print(mmse_tensor.shape)\n",
        "print(genders_tensor.shape)"
      ],
      "metadata": {
        "id": "-yE_k4IRlZbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Architecture (with Attention Mechanism)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Block 1 (Conv Layer)\n",
        "        self.block1_conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.block1_conv2 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Attention Layer (for EEG data)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=64, num_heads=4, dropout=0.1)\n",
        "\n",
        "        # Block 2 (Conv Layer)\n",
        "        self.block2_conv1 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.block2_conv2 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 + 3, 512)  # 128 from Conv output + 3 metadata features\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x, age, mmse, gender):\n",
        "        # Block 1 (Convolutional Layers)\n",
        "        x = F.relu(self.block1_conv1(x))\n",
        "        x = F.relu(self.block1_conv2(x))\n",
        "\n",
        "        # Attention mechanism\n",
        "        x = x.permute(2, 0, 1)  # (batch, channels, seq_len) → (seq_len, batch, channels)\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        x = attn_output.permute(1, 2, 0)  # back to (batch, channels, seq_len)\n",
        "\n",
        "        # Block 2 (Convolutional Layers)\n",
        "        x = F.relu(self.block2_conv1(x))\n",
        "        x = F.relu(self.block2_conv2(x))\n",
        "\n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x).squeeze(-1)  # Shape: (batch_size, 128)\n",
        "\n",
        "        # Flatten metadata\n",
        "        age = age.view(age.size(0), -1)\n",
        "        mmse = mmse.view(mmse.size(0), -1)\n",
        "        gender = gender.view(gender.size(0), -1)\n",
        "\n",
        "        # Concatenate all features\n",
        "        combined_features = torch.cat([x, age, mmse, gender], dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(combined_features))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Data Augmentation (add noise to input data)\n",
        "def augment_data(x, noise_factor=0.1):\n",
        "    # Adding random noise to the input\n",
        "    noise = torch.randn_like(x) * noise_factor\n",
        "    return x + noise"
      ],
      "metadata": {
        "id": "K3-ueuzClleB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Leave-One-Patient-Out Cross-Validation\n",
        "patients = np.unique(patient_ids)\n",
        "accuracies = []\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for patient in patients:\n",
        "    # Split the data into training and testing based on patient ID\n",
        "    train_mask = patient_ids != patient\n",
        "    test_mask = patient_ids == patient\n",
        "\n",
        "    # Prepare training and testing data\n",
        "    X_train, y_train = epochs_normalized[train_mask], groups[train_mask]\n",
        "    X_test, y_test = epochs_normalized[test_mask], groups[test_mask]\n",
        "\n",
        "    # Prepare the additional features (age, MMSE, gender) for training and testing\n",
        "    age_train, mmse_train, gender_train = ages_tensor[train_mask], mmse_tensor[train_mask], genders_tensor[train_mask]\n",
        "    age_test, mmse_test, gender_test = ages_tensor[test_mask], mmse_tensor[test_mask], genders_tensor[test_mask]\n",
        "\n",
        "    # Convert to tensors and move to the device\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "    age_train_tensor = age_train.to(device)\n",
        "    mmse_train_tensor = mmse_train.to(device)\n",
        "    gender_train_tensor = gender_train.to(device)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "    age_test_tensor = age_test.to(device)\n",
        "    mmse_test_tensor = mmse_test.to(device)\n",
        "    gender_test_tensor = gender_test.to(device)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor, age_train_tensor, mmse_train_tensor, gender_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor, age_test_tensor, mmse_test_tensor, gender_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = CNN(input_channels=19, num_classes=3).to(device)\n",
        "\n",
        "    # Define loss function and optimizer with weight decay\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 regularization\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate scheduler\n",
        "\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for epoch in range(5):  # Increase this number to 10-50 for better training\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels, age, mmse, gender in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            age, mmse, gender = age.to(device), mmse.to(device), gender.to(device)\n",
        "\n",
        "            # Apply augmentation here\n",
        "            inputs = augment_data(inputs)\n",
        "            # print(inputs.shape)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, age, mmse, gender)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()  # Step the learning rate scheduler\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, age, mmse, gender in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            age, mmse, gender = age.to(device), mmse.to(device), gender.to(device)\n",
        "            outputs = model(inputs, age, mmse, gender)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Collect predictions and labels for later metric calculations\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Patient {patient}: Test Accuracy: {accuracy}%\")\n",
        "\n",
        "# Calculate metrics after loop\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "# Print overall performance metrics\n",
        "print(f\"Average Accuracy: {np.mean(accuracies)}%\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "MooaPGhWl6vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "group_accuracies = defaultdict(list)\n",
        "\n",
        "for i, patient in enumerate(patients):\n",
        "    patient_group = groups[patient_ids == patient][0]  # All epochs for a patient have the same group\n",
        "    group_accuracies[patient_group].append(accuracies[i])\n",
        "\n",
        "# Print mean accuracy per group\n",
        "for group, accs in group_accuracies.items():\n",
        "    print(f\"Group {group} Mean Accuracy: {np.mean(accs):.2f}% (n={len(accs)})\")"
      ],
      "metadata": {
        "id": "cx8-EJ4Slq4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}